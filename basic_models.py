import pickle
import argparse

#An n-gram model is a probabilistic language model used in natural language processing (NLP) and text analysis. It's based on the idea that the probability of a word occurring in a sequence depends not only on the previous word but also on the previous (n-1) words.
def features(train_text, val_text, test_text, max_ngram = 2):
#n-gram features generated by techniques such as CountVectorizer or TfidfVectorizer can be used as input features for various machine learning models, including Support Vector Machines (SVMs) like SVC (Support Vector Classifier).

def basic_model(x, y, val_x, val_y, test_x, test_y, model = 'svc'):
    if flag == 'svc':
        model = sklearn.svm.SVC(probability = True)
    elif flag == 'linear svc':
        model = sklearn.svm.LinearSVC()
    elif flag == 'logistic':
        model = sklearn.linear_model.LogisticRegression(n_jobs = 2)
    elif flag == 'naive bayes':
        model = sklearn.naive_bayes.MultinomiaNB()


    model.fit(X, Y)
    test_y_est = classifier.predict(test_x)
    clssifier_report = sklearn.matrics.classification_report(test_y, test_y_est, digits = 3)
    accuracy = sklearn.metrics.accuracy_score(test_y, test_y_est)
    auc_score = sklearn.metrics.roc_auc_score(test_y, test_y_est)
    #plot ROC
    false_posi_rate, true_posi_rate, thresholds = sklean.metrics.roc_curve(test_y, test_y_est)
    return accuracy, auc_score



def main():

    global args
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('--data', help="The input file", type=str, default='train_val.pkl')
    parser.add_argument('--ngram', help="Maximum ngram length", type=int, default=3)
    parser.add_argument('--model', help="model name", type=str, default='svc')

    args = parser.parse_args()
 
    #Load data from pickle file
    file_path = 'data.pickle'
    train_text, train_cond, val_text, val_cond, test_text, test_cond = pd.read_pickle(args.data)
    accuracies = {}
    aucs = {}

    conditions = ['cohort', 
                  'Obesity', 
                  'Non_Adherence', 
                  'Developmental_Delay_Retardation',
                  'Advanced_Heart_Disease',
                  'Advanced_Lung_Disease', 
                  'Schizophrenia_and_other_Psychiatric_Disorders', 
                  'Alcohol_Abuse', 
                  'Other_Substance_Abuse', 
                  'Chronic_Pain_Fibromyalgia', 
                  'Chronic_Neurological_Dystrophies', 
                  'Advanced_Cancer', 
                  'Depression', 
                  'Dementia', 
                  'Unsure'] 



    for i, j in enumerate(conditions):
        print("Start to working on condition {}".format(j))
        train_y = train_cond[:, i]
        test_y = test_cond[:, i]
        accuracy, auc_score = basic_model(train)
    # with open("args.data", 'rb') as f:
    #     data = pickle.load(f)

    # Display the loaded data
    for line in data[:5]:
        print(line)


    # all_accs = []
    # all_aucs = []

    # with h5py.File(args.data, "r") as f:
    #     if not os.path.isfile(os.path.join("converted/X_features-"+str(args.ngram)+".npz")):
    #         print('Computing features.')
    #         train_x = f["train"][:]
    #         valid_x = f["val"][:]
    #         test_x = f["test"][:]

    #         X_features, val_X_features, test_X_features = extract_features(train_x, valid_x, test_x, args.ngram)
    #     else:
    #         print('Skipping Build of Ngrams: model already exists.')
    #         X_features = dict(np.load(os.path.join("converted", "X_features-"+str(args.ngram)+".npz")))['features'].item()
    #         val_X_features = dict(np.load(os.path.join("converted", "val_X_features-"+str(args.ngram)+".npz")))['features'].item()
    #         test_X_features = dict(np.load(os.path.join("converted", "test_X_features-"+str(args.ngram)+".npz")))['features'].item()




    #     for index, condition in enumerate(conditions):
    #         # for dataset_filepath in sorted(glob.glob(os.path.join(data_folder_formatted, 'icu_frequent_flyers_cohort.npz'))):

    #         print('Current Condition: {0}'.format(condition))

    #         train_y = f["train_label"][:,index]
    #         valid_y = f["val_label"][:,index]
    #         test_y = f["test_label"][:,index]
    #         current_accs = []
    #         current_aucs = []
    #         #for subset in xrange(1,21):
    #         #    s = subset/float(20)
    #         acc, auc = make_predictions(X_features, train_y, val_X_features, valid_y, test_X_features, test_y, 1)#s)
    #         current_accs.append(acc)
    #         current_aucs.append(auc)
    #         print('\n')
          

if __name__ == "__main__":
    main()
